---
output:
  html_document: default
  pdf_document:
    fig_caption: null
    keep_md: yes
  word_document: default
---

Title: Prediction Assignment - Practical Machine Learning

Project Introduction and goal :

In this project, the goal is using device data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants to quantify how well a particular activity is done.   More information is available from the website here: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

Data :

The training & test data for this project are available here: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Required Libraries:

```{r figA, echo=TRUE, fig=TRUE, fig.path="figure/"}
## Data Processing

# Getting the data and Loading them

library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(knitr)


set.seed(10000)

trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

training <- read.csv(url(trainUrl), na.strings=c("NA","#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("NA","#DIV/0!",""))

inTrain <- createDataPartition(training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]
myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)

#Cleaning and removing first column of the dataset :

nzv <- nearZeroVar(myTraining, saveMetrics=TRUE)
myTraining <- myTraining[,nzv$nzv==FALSE]

nzv<- nearZeroVar(myTesting,saveMetrics=TRUE)
myTesting <- myTesting[,nzv$nzv==FALSE]

myTraining <- myTraining[c(-1)]


#Discarding variables with more than 60% NA

trainingV3 <- myTraining
for(i in 1:length(myTraining)) {
    if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .7) {
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  {
                trainingV3 <- trainingV3[ , -j]
            }   }   } }

# Set back to the original variable name
myTraining <- trainingV3
rm(trainingV3)

#Transform the myTesting and testing data sets

clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58])  # remove the classe column

#Making training and testing dataset equivalent columns

myTesting <- myTesting[clean1]         
testing <- testing[clean2]          

dim(myTesting)

# Coerce the data into same type

for (i in 1:length(testing) ) {
    for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) == 1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}

# To get the same class between testing and myTraining
testing <- rbind(myTraining[2, -58] , testing)
testing <- testing[-1,]


## Predictive Analytics using Decision Tree

set.seed(10000)
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
#fancyRpartPlot(modFitA1)


predictionsA1 <- predict(modFitA1, myTesting, type = "class")
cmtree <- confusionMatrix(predictionsA1, myTesting$classe)
cmtree

#Plotting the decision tree

plot(cmtree$table, col = cmtree$byClass, main = paste("Decision Tree Confusion Matrix: Accuracy =", round(cmtree$overall['Accuracy'], 4)))


## Predictive Analytics using Random Forests

set.seed(10000)
modFitB1 <- randomForest(classe ~ ., data=myTraining)
predictionB1 <- predict(modFitB1, myTesting, type = "class")
cmrf <- confusionMatrix(predictionB1, myTesting$classe)
cmrf

plot(modFitB1)

#Plotting for the random forest:


plot(cmrf$table, col = cmtree$byClass, main = paste("Random Forest Confusion Matrix: Accuracy =", round(cmrf$overall['Accuracy'], 4)))


## Predicting Results on the Test Data

#Random Forests gave an Accuracy in the myTesting dataset of 99.89%, which was more accurate that what I got from the Decision Trees. The expected out-of-sample error is 100-99.89 = 0.11%.

predictionB2 <- predict(modFitB1, testing, type = "class")
predictionB2

